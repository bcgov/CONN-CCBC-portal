nameOverride: ccbc
fullnameOverride: ccbc

replicaCount: 3

image:
  app:
    repository: ghcr.io/bcgov/conn-ccbc-portal/ccbc-app
    pullPolicy: IfNotPresent
    tag: 'sha-d53715434b85fd10f07e9976014e2dfa6166b818' # The tag value must be passed in via the deploy script
    clientSecret: '' # clientSecret must be passed in via deploy script
  psql:
    repository: gcr.io/ggl-cas-storage/cas-postgres
    pullPolicy: IfNotPresent
    tag: '0.3.0'
  db:
    repository: ghcr.io/bcgov/conn-ccbc-portal/ccbc-db
    pullPolicy: IfNotPresent
    tag: 'sha-d53715434b85fd10f07e9976014e2dfa6166b818' # The tag value must be passed in via the deploy script

app:
  port: '3000'
  probesPort: '9000'
  namespace: ''
  enableAnalytics: true
secureRoute:
  host: '' # The host value must be passed in via the deploy script
  wwwRoute:
    enable: false
growthbook:
  key: '' # The host value must be passed in via the deploy script
resources:
  app:
    requests:
      cpu: 1m
      memory: '512Mi'
    limits:
      cpu: 500m
      memory: '1Gi'
  psql:
    requests:
      cpu: 10m
      memory: '128Mi'
    limits:
      cpu: 100m
      memory: '256Mi'

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 5
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

deployer:
  namespace: '' # The namespace must be passed in via the deploy script
  serviceAccount:
    enabled: true

networkPolicies:
  enabled: true

objectStorage:
  awsS3Bucket: '' # The value must be passed in via the deploy script
  awsS3Region: '' # The value must be passed in via the deploy script
  awsS3Key: '' # The value must be passed in via the deploy script
  awsS3SecretKey: '' # The value must be passed in via the deploy script
  awsRoleArn: '' # The value must be passed in via the deploy script

db:
  name: ccbc
  appUser: ccbc_app
  readonlyUser: ccbc_readonly

  # A bash script to run in a pre-upgrade hook, in a psql container
  # The script will be run by the postgres user,
  # allowing to clean up any objects created in a previous postMigrationCommand
  preUpgradeCommand: ~

  # A bash script to run after the migrations are deployed, in a psql container
  # The script will be run by the postgres user, allowing to deploy test data, bypassing RLS
  postMigrationCommand: ~

metabase:
  instanceName: ccbc-metabase
  namespace: '' # The value must be passed in via the deploy script
  prodNamespace: '' # The value must be passed in via the deploy script
  prodIngress:
    enable: false

certbot:
  image:
    tag: 1.0.0
    pullPolicy: IfNotPresent
  certbot:
    email: '' # The value must be passed in via the deploy script
    server: '' # The value must be passed in via the deploy script
  cron:
    suspend: false # The certs have been issued in all environments

loadTest:
  # if true, this will start the app with ENABLE_MOCK_AUTH=true, and deploy the data located in `db/data/perf`
  # These settings enable load testing, which is triggered in a separate helm chart
  enable: false

crunchyImage: artifacts.developer.gov.bc.ca/bcgov-docker-local/crunchy-postgres:ubi8-14.7-0

postgresVersion: 14

instances:
  name: ha # high availability
  replicas: 3
  dataVolumeClaimSpec:
    storage: 2Gi
    storageClassName: netapp-block-standard
  requests:
    cpu: 1m
    memory: 1Gi
  limits:
    cpu: 1000m
    memory: 1Gi
  replicaCertCopy:
    requests:
      cpu: 1m
      memory: 32Mi
    limits:
      cpu: 50m
      memory: 64Mi

pgBackRest:
  image: artifacts.developer.gov.bc.ca/bcgov-docker-local/crunchy-pgbackrest:ubi8-2.41-4
  retention: '30' # Ideally a larger number such as 30 backups/days
  # If retention-full-type set to 'count' then the oldest backups will expire when the number of backups reach the number defined in retention
  # If retention-full-type set to 'time' then the number defined in retention will take that many days worth of full backups before expiration
  retentionFullType: count
  repos:
    schedules:
      full: 0 8 * * *
      incremental: 0 0,4,12,16,20 * * *
    volume:
      accessModes: 'ReadWriteOnce'
      storage: 128Gi
      storageClassName: netapp-file-backup
  repoHost:
    requests:
      cpu: 2m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi
  sidecars:
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi

patroni:
  postgresql:
    pg_hba: 'host all all 0.0.0.0/0 md5'
    parameters:
      shared_buffers: 16MB # default is 128MB; a good tuned default for shared_buffers is 25% of the memory allocated to the pod
      wal_buffers: '64kB' # this can be set to -1 to automatically set as 1/32 of shared_buffers or 64kB, whichever is larger
      min_wal_size: 32MB
      max_wal_size: 64MB # default is 1GB
      max_slot_wal_keep_size: 128MB # default is -1, allowing unlimited wal growth when replicas fall behind

proxy:
  pgBouncer:
    image: artifacts.developer.gov.bc.ca/bcgov-docker-local/crunchy-pgbouncer:ubi8-1.18-0
    replicas: 2
    requests:
      cpu: 2m
      memory: 8Mi
    limits:
      cpu: 50m
      memory: 32Mi

pgmonitor:
  enabled: false
  exporter:
    image: artifacts.developer.gov.bc.ca/bcgov-docker-local/crunchy-postgres-exporter:ubi8-5.3.1-0
    requests:
      cpu: 1m
      memory: 64Mi
    limits:
      cpu: 50m
      memory: 128Mi
